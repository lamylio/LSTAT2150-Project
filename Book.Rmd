---
title: "Book"
author: "Lamy Lionel"
date: "02/12/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Setup
```{r}
# ---
# Please see setup.R
# ---

set.seed(2020)
source("./sources/setup.r")
source("./sources/minimization.r")
source("./sources/investigate.R")

# Global X and Y
CP.X = X(n)
CP.Y = Y(CP.X)
CP.x = seq(0, 1, length = n) 

CP.XO = sort(CP.X)
CP.YO = CP.Y[order(CP.X)]

# Kernel
Knorm = function(u) dnorm(u) #Gaussian kernel

# NW Estimator
NW.regEst <- function(x, X, Y, h, K) sum(Y * K((x - X)/h))/sum(K((x - X)/h))
```

# Finding best bandwidth and polynomial order

```{r}
# ---
# Please see minimize.R
# ---

# Warning: might take a while
# Use ni=100, M=100 by default but better results with higher values

# NW.MISE = NW.minimizeMISE()
# NW.MSSE = NW.minimizeMSSE()
# ---

NW.optimal = NW.minimizeMSSE(h.test = seq(0.035, 0.036, 0.00001), M=200) # more precise
LM.optimal = LM.minimizeMSSE(M=500) # can also use decimal as degree, but that's not "polynomial"

# Optimal bandwidth
NW.h = NW.optimal$h[1] # 0.0355
# Optimal degree
LM.poly = LM.optimal$poly[1] # 6
```

## Investigation of the optimal model
### Bias, Variance and MSSE

```{r}
# As X.optimal already contains the bias and the MSSE
# we only need to compute the variance, via MReg = \bar{x}

NW.optimal$variance = round(
  mean(
    (sapply(seq(0,1,length=nrow(NW.optimal)), function(x) NW.regEst(x, CP.X, CP.Y, NW.h, Knorm)) - NW.optimal$MReg)^2
  )
,4)

LM.optimal$variance = round(
  mean(
    (lm(CP.YO ~ poly(CP.XO, LM.poly))$fitted - LM.optimal$MReg)^2
  )
,4)

# head(NW.optimal, 1)
# head(LM.optimal, 1)
```

## Investigation at interesting points
### Of the MSE

```{r}
# ---
# Please see investigate.R
# ---

CP.to_check = c(0,0.2,0.45,0.63,0.78,0.91,1) 
CP.investigation.samples = c(25,50,100,200,400,600,800,1000) # smoother lines 

# Draw which points
plot(CP.X, CP.Y, pch="+", col = "grey", xlab = "x", ylab="Y")
lines(CP.x, m(CP.x), col = 1, lwd=2)
abline(v=CP.to_check, col=2, lty=3)
legend("topleft", 
    legend = c("True"),
    col = c(1),
    lty = c(1),
    cex = 1.5
)
title("Interesting points to investigate")

# Compute all the MSE
NW.investigation.results = CP.investigation.toframe(NW.investigation)
LM.investigation.results = CP.investigation.toframe(LM.investigation)

# Draw the evolution
CP.investigation.drawMSE(NW.investigation.results, "MSE evolution for NW")
CP.investigation.drawMSE(LM.investigation.results, "MSE evolution for LM")

```

# =====
#  END
# =====

# Plotting and comparison
```{r}
NW.h = 0.024 # MSSE.minimized[2]

n = 500
CP.x = seq(0,1,length=n)
CP.X = X(n)
CP.Y = Y(CP.X)

for (h in seq(0.0002, 0.05, 0.0002)){
  jpeg(paste0("./plots/NW-Line-100/NW-", h, ".jpg"), quality = 100, width = 1080, height = 720)
  plot(CP.X, CP.Y, pch="+", col = "grey")
  lines(CP.x, sapply(CP.x, function(x) NW.regEst(x, CP.X, CP.Y, h, Knorm)), col = 2, pch=19, lwd=1.5)
  #points(CP.x, sapply(CP.x, function(x) NW.regEst(x, CP.X, CP.Y, h, Kunif)), col = 4, pch=19)
  lines(CP.x, m(CP.x), col = 1, lwd=2)
  legend("topleft", 
    legend = c("True", "Gaussian"),
    col = c(1, 2, 4),
    lty = c(1, 0),
    pch = c(NA, 19),
    cex = 1.5
  )
  title(paste0("Nadaraya-watson regression with h=",h))
  dev.off()
  
}


```


```{r}
LM.poly = 15.31

plot(X, CP.Y, pch="+", col = "grey")
points(X, lm(CP.Y ~ poly(X,6.1))$fitted, col = 3, pch=16)
points(X, lm(CP.Y ~ poly(X,15))$fitted, col = 2, pch=18)
lines(x, m(x), col=1, lwd=1.5)
```

# Comparison between parametric and nonparam

```{r}
X = runif(100)
CP.Y = Y(X)


plot(X, CP.Y, pch="+", col = "grey")
points(x, sapply(x, function(x) NW.regEst(x, X, CP.Y, NW.h, Knorm)), col = 4, pch=20, pty=2)
#points(x, sapply(x, function(x) NW.regEst(x, X, CP.Y, NW.h, Kunif)), col = 4, pch=18)
points(X, lm(CP.Y ~ poly(X, LM.poly))$fitted, col = 2, pch=18) # LM
lines(x, m(x), col = 1, lwd=2)
legend("topleft", 
  legend = c("True", "NW Normal", "LM Poly"),
  col = c(1, 4, 2),
  lty = c(1, 1, 1), 
  cex = 0.5
)
  


```


```{r}
# Linear setup

h = 0.04

Knorm <- function(u) dnorm(u)

NW = function(x, X, Y, h, K) sum(Y * K((x - X)/h))/sum(K((x - X)/h))
NW.estimator = sapply(x, function(x) NW(x, X, Y, h, Knorm))
```

```{r}

# ndp = lm(Y ~ bs(X, intercept = T, knots = 4))

# Monte-Carlo

B = 1000
n = 100
x = seq(0, 1, length=n)
eps = 0.5*rnorm(n)

NW.boot = matrix(NA, B, n) 
for (i in 1:B){
  
  X = runif(n)
  Y = m(X) + eps
  h = 0.04 #h_NR(X, Knorm)
  NW.boot[i,] = sapply(x, function(x) NW(x, X, Y, h, Knorm))
}

NW.boot.mean = colMeans(NW.boot)
NW.boot.bias = NW.boot.mean - m(x)

NW.boot.MSE = (function(){
  SE = matrix(NA, B, n)
  for (i in 1:n) {
    SE[,i] = (NW.boot[,i]-m(x[i]))^2
  }
  return(colMeans(SE))
})()

NW.boot.MSSE = mean(NW.boot.MSE)

NW.boot.MISE = (function(){
  return (
    integrate(function(x) NW.boot.MSE[x] , 1, n, subdivisions = 1000)
  )
})()

print(NW.boot.MSSE)
print(NW.boot.MISE)

```

```{r}
# ndpConf = predict(ndp, interval = "conf")
plot(X, Y, pch="+", col = "grey")
# lines(x, ndpConf[,2], type = "l", col = 1)
lines(x, predict(smooth.spline(X, Y, cv = 3, spar = 0.6))$y, type = "l", col = 1, lwd=1.5)
lines(x, ndp$fitted, type = "l", col = 4, lwd=1.5)
# lines(x, ndpConf[,3], type = "l", col = 1)
lines(x, NWnormEst, type = "l", col = 2, lwd=1.5)
lines(x, m(x), type = "l", col = 3, lwd=1.5)
legend("topleft", 
  legend = c("LR", "NW", "True"),
  col = c(4, 2, 3),
  lty = c(1, 1, 1, 1), cex = 0.5)
```

